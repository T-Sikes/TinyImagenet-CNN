{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoOpktIFdWdf"
      },
      "source": [
        "# Data Processing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EP8wLB-04_GT",
        "outputId": "7e86a1fb-a83b-4c72-e860-635ba486a556"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "#reproducability\n",
        "import random, os\n",
        "import numpy as np\n",
        "import torch\n",
        "import pickle\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Device:\" , device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Mo2yRKeTGIS2",
        "outputId": "b87fdafa-4bae-4702-b2a5-aba8d60c85f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train images shape: (5775, 64, 64, 3)\n",
            "Train lables shape: 5775\n",
            "Number of classes: 15\n",
            "class names: [6, 22, 26, 28, 35, 57, 62, 70, 108, 139, 151, 163, 173, 188, 189]\n",
            "Val images shape: (825, 64, 64, 3)\n",
            "val labels shape: 825\n",
            "Number of classes: 15\n"
          ]
        }
      ],
      "source": [
        "\n",
        "train_path = os.path.join('Data', 'train-70_.pkl')\n",
        "val_path   = os.path.join('Data', 'validation-10_.pkl')\n",
        "\n",
        "#----Load training data-----#\n",
        "with open(train_path, 'rb') as f:\n",
        "    train_data = pickle.load(f)\n",
        "\n",
        "images_train = train_data['images']\n",
        "labels_train = train_data['labels']\n",
        "all_classes = sorted(list(set(labels_train)))  # 15 classes\n",
        "\n",
        "\n",
        "print(\"Train images shape:\", images_train.shape)\n",
        "print(\"Train lables shape:\", len(labels_train))\n",
        "print(\"Number of classes:\", len(all_classes))\n",
        "print(\"class names:\", all_classes)\n",
        "\n",
        "\n",
        "with open(val_path, 'rb') as f:\n",
        "    val_data = pickle.load(f)\n",
        "\n",
        "images_val = val_data['images']\n",
        "labels_val = val_data['labels']\n",
        "\n",
        "print(\"Val images shape:\", images_val.shape)\n",
        "print(\"val labels shape:\", len(labels_val))\n",
        "print(\"Number of classes:\", len(all_classes))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NPP4_gz4IyYZ",
        "outputId": "610006a8-7fbb-425c-9233-da433d80f976"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mapped train labels range: tensor(0) tensor(14)\n",
            "Mapped val labels range: tensor(0) tensor(14)\n"
          ]
        }
      ],
      "source": [
        "#convert data to pyTorch tensors for manipulation and modeling\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# ---- Map original labels to 0..num_classes-1 ----\n",
        "class_to_idx = {cls: i for i, cls in enumerate(all_classes)}\n",
        "\n",
        "train_labels_mapped = torch.tensor([class_to_idx[l] for l in labels_train]).long()\n",
        "val_labels_mapped   = torch.tensor([class_to_idx[l] for l in labels_val]).long()\n",
        "\n",
        "# Now you can use these in your dataset\n",
        "train_labels_tensor = train_labels_mapped\n",
        "val_labels_tensor   = val_labels_mapped\n",
        "\n",
        "print(\"Mapped train labels range:\", train_labels_tensor.min(), train_labels_tensor.max())\n",
        "print(\"Mapped val labels range:\", val_labels_tensor.min(), val_labels_tensor.max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Mp239wz_W_0z",
        "outputId": "252cf082-a702-4cb5-fa07-62db8cfc4ae3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Channel-wise mean: [0.47475025 0.4356516  0.38977522]\n",
            "Channel-wise std: [0.26744732 0.2618989  0.2737367 ]\n"
          ]
        }
      ],
      "source": [
        "#standardization of data\n",
        "#uses mean and std for RGB channels across training set\n",
        "train_images_float = images_train.astype(np.float32) / 255.0\n",
        "\n",
        "mean = train_images_float.mean((0,1,2))\n",
        "std = train_images_float.std((0,1,2))\n",
        "\n",
        "print(\"Channel-wise mean:\", mean)\n",
        "print(\"Channel-wise std:\", std)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oW_Rb_tHY_ub"
      },
      "outputs": [],
      "source": [
        "#data set class to wrap tensor\n",
        "#applies image augmentations\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.RandomCrop(64, padding = 4),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(8),\n",
        "   \n",
        "    transforms.ColorJitter(\n",
        "        brightness=0.15, \n",
        "        contrast=0.15, \n",
        "        saturation=0.15, \n",
        "        hue=0.03\n",
        "    ),\n",
        "    #transforms.RandomAffine(\n",
        "       # degrees=0,\n",
        "      #  translate=(0.15,0.15),\n",
        "     #   scale=(0.85, 1.15)\n",
        "    #),\n",
        "    transforms.RandomGrayscale(p=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean.tolist(), std.tolist())\n",
        "])\n",
        "\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean.tolist(), std=std.tolist())\n",
        "])\n",
        "\n",
        "class TinyImageNetDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Create datasets with transforms\n",
        "train_dataset = TinyImageNetDataset(images_train, train_labels_tensor, train_transform)\n",
        "val_dataset = TinyImageNetDataset(images_val, val_labels_tensor, val_transform)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OO3KXd7UdOep",
        "outputId": "121f9d34-c665-4b54-cd8c-e54aa73cf7e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train batch images shape: torch.Size([64, 3, 64, 64])\n",
            "Train batch labels shape: torch.Size([64])\n"
          ]
        }
      ],
      "source": [
        "# Data Loaders\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = True, #training data must be shuffled\n",
        "    num_workers = 0, #number of parallel data loaders\n",
        "    pin_memory = True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = False, #validation data does not need shuffing\n",
        "    num_workers = 0,\n",
        "    pin_memory = True\n",
        ")\n",
        "\n",
        "for images, labels in train_loader:\n",
        "    print(\"Train batch images shape:\", images.shape)\n",
        "    print(\"Train batch labels shape:\", labels.shape)\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3g8xUhbY9GX"
      },
      "source": [
        "# Model Helpers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HrahOlxkZT0d"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, pool=True, dropout=0.0, kernel_size=3):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size=kernel_size,\n",
        "            padding=kernel_size // 2  # keeps spatial size same\n",
        "        )\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.dropout = nn.Dropout2d(dropout) if dropout > 0 else nn.Identity()\n",
        "        self.pool = nn.MaxPool2d(2, 2) if pool else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.pool(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-O3vQ0AZBk3"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Ygf92rmPc4VC"
      },
      "outputs": [],
      "source": [
        "class theBestCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(theBestCNN, self).__init__()\n",
        "\n",
        "        # --- Convolutional blocks ---\n",
        "        self.conv1 = ConvBlock(3, 128, dropout=0.0, kernel_size=3) \n",
        "        self.conv2 = ConvBlock(128, 128, dropout=0.05, kernel_size=3, pool=False) \n",
        "\n",
        "        self.conv3 = ConvBlock(128, 256, dropout=0.1) \n",
        "        self.conv4 = ConvBlock(256, 256, dropout=0.1, pool=False) \n",
        "\n",
        "        self.conv5 = ConvBlock(256, 512, dropout=0.15) \n",
        "        self.conv6 = ConvBlock(512, 512, dropout=0.15, pool=False) \n",
        "\n",
        "        self.conv7 = ConvBlock(512, 1024, dropout = 0.2)  \n",
        "        self.conv8 = ConvBlock(1024, 1024, dropout = 0.2, pool=False) \n",
        "\n",
        "        # --- Global Average Pooling --#\n",
        "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "      \n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512,256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256,num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.conv5(x)\n",
        "        x = self.conv6(x)\n",
        "        x = self.conv7(x)\n",
        "        x = self.conv8(x)\n",
        "\n",
        "        x = self.gap(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "theBestCNN(\n",
            "  (conv1): ConvBlock(\n",
            "    (conv): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (dropout): Identity()\n",
            "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv2): ConvBlock(\n",
            "    (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (dropout): Dropout2d(p=0.05, inplace=False)\n",
            "    (pool): Identity()\n",
            "  )\n",
            "  (conv3): ConvBlock(\n",
            "    (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
            "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv4): ConvBlock(\n",
            "    (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
            "    (pool): Identity()\n",
            "  )\n",
            "  (conv5): ConvBlock(\n",
            "    (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (dropout): Dropout2d(p=0.15, inplace=False)\n",
            "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv6): ConvBlock(\n",
            "    (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (dropout): Dropout2d(p=0.15, inplace=False)\n",
            "    (pool): Identity()\n",
            "  )\n",
            "  (conv7): ConvBlock(\n",
            "    (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (dropout): Dropout2d(p=0.2, inplace=False)\n",
            "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv8): ConvBlock(\n",
            "    (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (dropout): Dropout2d(p=0.2, inplace=False)\n",
            "    (pool): Identity()\n",
            "  )\n",
            "  (gap): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Dropout(p=0.3, inplace=False)\n",
            "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Dropout(p=0.2, inplace=False)\n",
            "    (8): Linear(in_features=256, out_features=15, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "#instantiate model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = theBestCNN(num_classes=len(all_classes)).to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4Rn24lUggJLX"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Loss Function ---\n",
        "#loss function\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.04)\n",
        "\n",
        "#optimizer with l2 regularization (weight decay)\n",
        "optimizer = torch.optim.SGD(\n",
        "    model.parameters(),\n",
        "    lr=0.121,\n",
        "    momentum=0.9,\n",
        "    weight_decay=1.5e-4,\n",
        "    nesterov=True\n",
        ")\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=0.121,\n",
        "    epochs=150,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    pct_start=0.1,\n",
        "    anneal_strategy='cos',\n",
        "    div_factor=25.0,\n",
        "    final_div_factor=10000.0\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "collapsed": true,
        "id": "VVaF0bv2geY8",
        "outputId": "0aa8515f-3b6a-44e6-f508-963801c6bfc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/150] Train Loss: 2.6883, Train Acc: 0.0930 | Val Loss: 2.5422, Val Acc: 0.1442\n",
            "‚úì Saved Best Model! Best Val Acc: 0.1442\n",
            "Epoch [2/150] Train Loss: 2.5101, Train Acc: 0.1396 | Val Loss: 2.4558, Val Acc: 0.2097\n",
            "‚úì Saved Best Model! Best Val Acc: 0.2097\n",
            "Epoch [3/150] Train Loss: 2.3505, Train Acc: 0.1790 | Val Loss: 2.1601, Val Acc: 0.3248\n",
            "‚úì Saved Best Model! Best Val Acc: 0.3248\n",
            "Epoch [4/150] Train Loss: 2.2392, Train Acc: 0.1863 | Val Loss: 2.0405, Val Acc: 0.3636\n",
            "‚úì Saved Best Model! Best Val Acc: 0.3636\n",
            "Epoch [5/150] Train Loss: 2.2301, Train Acc: 0.1881 | Val Loss: 2.0390, Val Acc: 0.3697\n",
            "‚úì Saved Best Model! Best Val Acc: 0.3697\n",
            "Epoch [6/150] Train Loss: 2.1315, Train Acc: 0.1948 | Val Loss: 1.9309, Val Acc: 0.4048\n",
            "‚úì Saved Best Model! Best Val Acc: 0.4048\n",
            "Epoch [7/150] Train Loss: 2.1080, Train Acc: 0.1941 | Val Loss: 1.9594, Val Acc: 0.4145\n",
            "‚úì Saved Best Model! Best Val Acc: 0.4145\n",
            "Epoch [8/150] Train Loss: 2.0656, Train Acc: 0.2322 | Val Loss: 1.9671, Val Acc: 0.3964\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [9/150] Train Loss: 2.0377, Train Acc: 0.2426 | Val Loss: 1.7839, Val Acc: 0.4703\n",
            "‚úì Saved Best Model! Best Val Acc: 0.4703\n",
            "Epoch [10/150] Train Loss: 1.9712, Train Acc: 0.2436 | Val Loss: 1.9673, Val Acc: 0.4339\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [11/150] Train Loss: 1.8910, Train Acc: 0.3200 | Val Loss: 1.7153, Val Acc: 0.4873\n",
            "‚úì Saved Best Model! Best Val Acc: 0.4873\n",
            "Epoch [12/150] Train Loss: 1.9175, Train Acc: 0.2544 | Val Loss: 1.6320, Val Acc: 0.5030\n",
            "‚úì Saved Best Model! Best Val Acc: 0.5030\n",
            "Epoch [13/150] Train Loss: 1.9140, Train Acc: 0.2715 | Val Loss: 1.6569, Val Acc: 0.4715\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [14/150] Train Loss: 1.8413, Train Acc: 0.2944 | Val Loss: 1.7200, Val Acc: 0.4582\n",
            "No improvement. Patience: 2/18\n",
            "Epoch [15/150] Train Loss: 1.7968, Train Acc: 0.3006 | Val Loss: 1.5548, Val Acc: 0.5370\n",
            "‚úì Saved Best Model! Best Val Acc: 0.5370\n",
            "Epoch [16/150] Train Loss: 1.8012, Train Acc: 0.2727 | Val Loss: 1.6919, Val Acc: 0.5176\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [17/150] Train Loss: 1.7238, Train Acc: 0.3003 | Val Loss: 1.5129, Val Acc: 0.5455\n",
            "‚úì Saved Best Model! Best Val Acc: 0.5455\n",
            "Epoch [18/150] Train Loss: 1.7494, Train Acc: 0.3025 | Val Loss: 1.5119, Val Acc: 0.5418\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [19/150] Train Loss: 1.6749, Train Acc: 0.3094 | Val Loss: 1.4667, Val Acc: 0.5733\n",
            "‚úì Saved Best Model! Best Val Acc: 0.5733\n",
            "Epoch [20/150] Train Loss: 1.6226, Train Acc: 0.2708 | Val Loss: 1.4698, Val Acc: 0.5588\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [21/150] Train Loss: 1.5942, Train Acc: 0.3119 | Val Loss: 1.5081, Val Acc: 0.5661\n",
            "No improvement. Patience: 2/18\n",
            "Epoch [22/150] Train Loss: 1.6043, Train Acc: 0.3394 | Val Loss: 1.4150, Val Acc: 0.5927\n",
            "‚úì Saved Best Model! Best Val Acc: 0.5927\n",
            "Epoch [23/150] Train Loss: 1.6567, Train Acc: 0.3345 | Val Loss: 1.4199, Val Acc: 0.5842\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [24/150] Train Loss: 1.5856, Train Acc: 0.3678 | Val Loss: 1.3465, Val Acc: 0.6133\n",
            "‚úì Saved Best Model! Best Val Acc: 0.6133\n",
            "Epoch [25/150] Train Loss: 1.6274, Train Acc: 0.3120 | Val Loss: 1.4181, Val Acc: 0.6085\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [26/150] Train Loss: 1.5889, Train Acc: 0.3285 | Val Loss: 1.3368, Val Acc: 0.6133\n",
            "No improvement. Patience: 2/18\n",
            "Epoch [27/150] Train Loss: 1.5862, Train Acc: 0.3418 | Val Loss: 1.3402, Val Acc: 0.6388\n",
            "‚úì Saved Best Model! Best Val Acc: 0.6388\n",
            "Epoch [28/150] Train Loss: 1.5125, Train Acc: 0.3468 | Val Loss: 1.4358, Val Acc: 0.5721\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [29/150] Train Loss: 1.4378, Train Acc: 0.3110 | Val Loss: 1.2300, Val Acc: 0.6509\n",
            "‚úì Saved Best Model! Best Val Acc: 0.6509\n",
            "Epoch [30/150] Train Loss: 1.4715, Train Acc: 0.3541 | Val Loss: 1.3394, Val Acc: 0.6230\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [31/150] Train Loss: 1.4585, Train Acc: 0.3032 | Val Loss: 1.2317, Val Acc: 0.6642\n",
            "‚úì Saved Best Model! Best Val Acc: 0.6642\n",
            "Epoch [32/150] Train Loss: 1.4875, Train Acc: 0.3647 | Val Loss: 1.2682, Val Acc: 0.6485\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [33/150] Train Loss: 1.4471, Train Acc: 0.3785 | Val Loss: 1.2389, Val Acc: 0.6533\n",
            "No improvement. Patience: 2/18\n",
            "Epoch [34/150] Train Loss: 1.4249, Train Acc: 0.3430 | Val Loss: 1.1454, Val Acc: 0.6933\n",
            "‚úì Saved Best Model! Best Val Acc: 0.6933\n",
            "Epoch [35/150] Train Loss: 1.4770, Train Acc: 0.3638 | Val Loss: 1.2393, Val Acc: 0.6521\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [36/150] Train Loss: 1.3380, Train Acc: 0.3588 | Val Loss: 1.2137, Val Acc: 0.6703\n",
            "No improvement. Patience: 2/18\n",
            "Epoch [37/150] Train Loss: 1.4673, Train Acc: 0.3835 | Val Loss: 1.3181, Val Acc: 0.6424\n",
            "No improvement. Patience: 3/18\n",
            "Epoch [38/150] Train Loss: 1.3751, Train Acc: 0.4012 | Val Loss: 1.1796, Val Acc: 0.6897\n",
            "No improvement. Patience: 4/18\n",
            "Epoch [39/150] Train Loss: 1.3301, Train Acc: 0.3562 | Val Loss: 1.1264, Val Acc: 0.6752\n",
            "No improvement. Patience: 5/18\n",
            "Epoch [40/150] Train Loss: 1.3437, Train Acc: 0.3782 | Val Loss: 1.1993, Val Acc: 0.6800\n",
            "No improvement. Patience: 6/18\n",
            "Epoch [41/150] Train Loss: 1.3277, Train Acc: 0.3401 | Val Loss: 1.1026, Val Acc: 0.7139\n",
            "‚úì Saved Best Model! Best Val Acc: 0.7139\n",
            "Epoch [42/150] Train Loss: 1.2407, Train Acc: 0.4663 | Val Loss: 1.1165, Val Acc: 0.7042\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [43/150] Train Loss: 1.3265, Train Acc: 0.4185 | Val Loss: 1.1198, Val Acc: 0.6921\n",
            "No improvement. Patience: 2/18\n",
            "Epoch [44/150] Train Loss: 1.3242, Train Acc: 0.3413 | Val Loss: 1.1581, Val Acc: 0.6885\n",
            "No improvement. Patience: 3/18\n",
            "Epoch [45/150] Train Loss: 1.2336, Train Acc: 0.4410 | Val Loss: 1.0927, Val Acc: 0.7127\n",
            "No improvement. Patience: 4/18\n",
            "Epoch [46/150] Train Loss: 1.1720, Train Acc: 0.4606 | Val Loss: 1.0565, Val Acc: 0.7236\n",
            "‚úì Saved Best Model! Best Val Acc: 0.7236\n",
            "Epoch [47/150] Train Loss: 1.2085, Train Acc: 0.4074 | Val Loss: 1.0462, Val Acc: 0.7273\n",
            "‚úì Saved Best Model! Best Val Acc: 0.7273\n",
            "Epoch [48/150] Train Loss: 1.2742, Train Acc: 0.4145 | Val Loss: 1.0922, Val Acc: 0.7152\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [49/150] Train Loss: 1.2786, Train Acc: 0.3643 | Val Loss: 1.1582, Val Acc: 0.6885\n",
            "No improvement. Patience: 2/18\n",
            "Epoch [50/150] Train Loss: 1.2612, Train Acc: 0.4289 | Val Loss: 1.0597, Val Acc: 0.7079\n",
            "No improvement. Patience: 3/18\n",
            "Epoch [51/150] Train Loss: 1.2203, Train Acc: 0.3169 | Val Loss: 1.1008, Val Acc: 0.7042\n",
            "No improvement. Patience: 4/18\n",
            "Epoch [52/150] Train Loss: 1.2428, Train Acc: 0.4002 | Val Loss: 1.0607, Val Acc: 0.7200\n",
            "No improvement. Patience: 5/18\n",
            "Epoch [53/150] Train Loss: 1.2305, Train Acc: 0.4258 | Val Loss: 1.0514, Val Acc: 0.7188\n",
            "No improvement. Patience: 6/18\n",
            "Epoch [54/150] Train Loss: 1.1506, Train Acc: 0.4805 | Val Loss: 1.0505, Val Acc: 0.7188\n",
            "No improvement. Patience: 7/18\n",
            "Epoch [55/150] Train Loss: 1.1063, Train Acc: 0.4395 | Val Loss: 1.0382, Val Acc: 0.7212\n",
            "No improvement. Patience: 8/18\n",
            "Epoch [56/150] Train Loss: 1.1631, Train Acc: 0.4594 | Val Loss: 0.9688, Val Acc: 0.7442\n",
            "‚úì Saved Best Model! Best Val Acc: 0.7442\n",
            "Epoch [57/150] Train Loss: 1.1921, Train Acc: 0.4417 | Val Loss: 1.0292, Val Acc: 0.7345\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [58/150] Train Loss: 1.1802, Train Acc: 0.4255 | Val Loss: 0.9824, Val Acc: 0.7552\n",
            "‚úì Saved Best Model! Best Val Acc: 0.7552\n",
            "Epoch [59/150] Train Loss: 1.0704, Train Acc: 0.3714 | Val Loss: 1.0020, Val Acc: 0.7648\n",
            "‚úì Saved Best Model! Best Val Acc: 0.7648\n",
            "Epoch [60/150] Train Loss: 1.0487, Train Acc: 0.4559 | Val Loss: 1.0736, Val Acc: 0.7139\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [61/150] Train Loss: 1.1020, Train Acc: 0.4331 | Val Loss: 1.0085, Val Acc: 0.7479\n",
            "No improvement. Patience: 2/18\n",
            "Epoch [62/150] Train Loss: 1.0641, Train Acc: 0.4970 | Val Loss: 1.0274, Val Acc: 0.7261\n",
            "No improvement. Patience: 3/18\n",
            "Epoch [63/150] Train Loss: 0.9659, Train Acc: 0.4868 | Val Loss: 1.0196, Val Acc: 0.7345\n",
            "No improvement. Patience: 4/18\n",
            "Epoch [64/150] Train Loss: 1.0343, Train Acc: 0.4679 | Val Loss: 0.9711, Val Acc: 0.7576\n",
            "No improvement. Patience: 5/18\n",
            "Epoch [65/150] Train Loss: 0.9568, Train Acc: 0.4582 | Val Loss: 1.0043, Val Acc: 0.7406\n",
            "No improvement. Patience: 6/18\n",
            "Epoch [66/150] Train Loss: 1.0491, Train Acc: 0.4888 | Val Loss: 0.9789, Val Acc: 0.7685\n",
            "‚úì Saved Best Model! Best Val Acc: 0.7685\n",
            "Epoch [67/150] Train Loss: 0.9650, Train Acc: 0.4828 | Val Loss: 0.9405, Val Acc: 0.7661\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [68/150] Train Loss: 1.0154, Train Acc: 0.4880 | Val Loss: 1.0177, Val Acc: 0.7539\n",
            "No improvement. Patience: 2/18\n",
            "Epoch [69/150] Train Loss: 0.9807, Train Acc: 0.4916 | Val Loss: 0.9526, Val Acc: 0.7685\n",
            "No improvement. Patience: 3/18\n",
            "Epoch [70/150] Train Loss: 1.0129, Train Acc: 0.4215 | Val Loss: 1.0072, Val Acc: 0.7455\n",
            "No improvement. Patience: 4/18\n",
            "Epoch [71/150] Train Loss: 1.1315, Train Acc: 0.4332 | Val Loss: 0.9472, Val Acc: 0.7648\n",
            "No improvement. Patience: 5/18\n",
            "Epoch [72/150] Train Loss: 1.0744, Train Acc: 0.3953 | Val Loss: 0.9488, Val Acc: 0.7673\n",
            "No improvement. Patience: 6/18\n",
            "Epoch [73/150] Train Loss: 0.9203, Train Acc: 0.4655 | Val Loss: 0.9238, Val Acc: 0.7697\n",
            "‚úì Saved Best Model! Best Val Acc: 0.7697\n",
            "Epoch [74/150] Train Loss: 0.9987, Train Acc: 0.4192 | Val Loss: 0.9951, Val Acc: 0.7418\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [75/150] Train Loss: 0.9759, Train Acc: 0.4954 | Val Loss: 1.0012, Val Acc: 0.7394\n",
            "No improvement. Patience: 2/18\n",
            "Epoch [76/150] Train Loss: 1.0353, Train Acc: 0.4845 | Val Loss: 0.9523, Val Acc: 0.7600\n",
            "No improvement. Patience: 3/18\n",
            "Epoch [77/150] Train Loss: 1.0022, Train Acc: 0.4516 | Val Loss: 0.9533, Val Acc: 0.7697\n",
            "No improvement. Patience: 4/18\n",
            "Epoch [78/150] Train Loss: 0.9866, Train Acc: 0.4159 | Val Loss: 0.9823, Val Acc: 0.7612\n",
            "No improvement. Patience: 5/18\n",
            "Epoch [79/150] Train Loss: 0.9642, Train Acc: 0.5027 | Val Loss: 0.9550, Val Acc: 0.7673\n",
            "No improvement. Patience: 6/18\n",
            "Epoch [80/150] Train Loss: 1.0546, Train Acc: 0.5138 | Val Loss: 0.9366, Val Acc: 0.7830\n",
            "‚úì Saved Best Model! Best Val Acc: 0.7830\n",
            "Epoch [81/150] Train Loss: 1.0222, Train Acc: 0.4653 | Val Loss: 0.9229, Val Acc: 0.7842\n",
            "‚úì Saved Best Model! Best Val Acc: 0.7842\n",
            "Epoch [82/150] Train Loss: 0.9372, Train Acc: 0.5254 | Val Loss: 0.9824, Val Acc: 0.7685\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [83/150] Train Loss: 0.7789, Train Acc: 0.5162 | Val Loss: 0.9632, Val Acc: 0.7721\n",
            "No improvement. Patience: 2/18\n",
            "Epoch [84/150] Train Loss: 0.9046, Train Acc: 0.5216 | Val Loss: 0.9160, Val Acc: 0.7867\n",
            "‚úì Saved Best Model! Best Val Acc: 0.7867\n",
            "Epoch [85/150] Train Loss: 0.9275, Train Acc: 0.4481 | Val Loss: 0.9748, Val Acc: 0.7527\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [86/150] Train Loss: 0.9348, Train Acc: 0.5484 | Val Loss: 0.9378, Val Acc: 0.7709\n",
            "No improvement. Patience: 2/18\n",
            "Epoch [87/150] Train Loss: 0.8379, Train Acc: 0.5089 | Val Loss: 0.9543, Val Acc: 0.7685\n",
            "No improvement. Patience: 3/18\n",
            "Epoch [88/150] Train Loss: 0.9856, Train Acc: 0.4970 | Val Loss: 0.9237, Val Acc: 0.7818\n",
            "No improvement. Patience: 4/18\n",
            "Epoch [89/150] Train Loss: 0.8523, Train Acc: 0.4956 | Val Loss: 0.9026, Val Acc: 0.7842\n",
            "No improvement. Patience: 5/18\n",
            "Epoch [90/150] Train Loss: 0.8835, Train Acc: 0.5120 | Val Loss: 0.9542, Val Acc: 0.7758\n",
            "No improvement. Patience: 6/18\n",
            "Epoch [91/150] Train Loss: 0.8583, Train Acc: 0.3803 | Val Loss: 0.9489, Val Acc: 0.7685\n",
            "No improvement. Patience: 7/18\n",
            "Epoch [92/150] Train Loss: 0.8059, Train Acc: 0.5032 | Val Loss: 0.9544, Val Acc: 0.7721\n",
            "No improvement. Patience: 8/18\n",
            "Epoch [93/150] Train Loss: 0.7769, Train Acc: 0.4362 | Val Loss: 0.9196, Val Acc: 0.7782\n",
            "No improvement. Patience: 9/18\n",
            "Epoch [94/150] Train Loss: 0.9153, Train Acc: 0.5373 | Val Loss: 0.9303, Val Acc: 0.7721\n",
            "No improvement. Patience: 10/18\n",
            "Epoch [95/150] Train Loss: 0.9065, Train Acc: 0.4655 | Val Loss: 0.9242, Val Acc: 0.7758\n",
            "No improvement. Patience: 11/18\n",
            "Epoch [96/150] Train Loss: 0.8123, Train Acc: 0.5565 | Val Loss: 0.9293, Val Acc: 0.7721\n",
            "No improvement. Patience: 12/18\n",
            "Epoch [97/150] Train Loss: 0.8391, Train Acc: 0.4239 | Val Loss: 0.8901, Val Acc: 0.7867\n",
            "No improvement. Patience: 13/18\n",
            "Epoch [98/150] Train Loss: 0.7959, Train Acc: 0.4817 | Val Loss: 0.9273, Val Acc: 0.7806\n",
            "No improvement. Patience: 14/18\n",
            "Epoch [99/150] Train Loss: 0.8040, Train Acc: 0.5503 | Val Loss: 0.8773, Val Acc: 0.7988\n",
            "‚úì Saved Best Model! Best Val Acc: 0.7988\n",
            "Epoch [100/150] Train Loss: 0.8936, Train Acc: 0.5231 | Val Loss: 0.9264, Val Acc: 0.7745\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [101/150] Train Loss: 0.8636, Train Acc: 0.4864 | Val Loss: 0.9474, Val Acc: 0.7758\n",
            "No improvement. Patience: 2/18\n",
            "Epoch [102/150] Train Loss: 0.7961, Train Acc: 0.4260 | Val Loss: 0.9474, Val Acc: 0.7721\n",
            "No improvement. Patience: 3/18\n",
            "Epoch [103/150] Train Loss: 0.7229, Train Acc: 0.5016 | Val Loss: 0.9060, Val Acc: 0.7806\n",
            "No improvement. Patience: 4/18\n",
            "Epoch [104/150] Train Loss: 0.6842, Train Acc: 0.5482 | Val Loss: 0.9348, Val Acc: 0.7745\n",
            "No improvement. Patience: 5/18\n",
            "Epoch [105/150] Train Loss: 0.7860, Train Acc: 0.5640 | Val Loss: 0.9240, Val Acc: 0.7770\n",
            "No improvement. Patience: 6/18\n",
            "Epoch [106/150] Train Loss: 0.7609, Train Acc: 0.4899 | Val Loss: 0.9135, Val Acc: 0.7794\n",
            "No improvement. Patience: 7/18\n",
            "Epoch [107/150] Train Loss: 0.7973, Train Acc: 0.5732 | Val Loss: 0.9359, Val Acc: 0.7806\n",
            "No improvement. Patience: 8/18\n",
            "Epoch [108/150] Train Loss: 0.6875, Train Acc: 0.5467 | Val Loss: 0.9346, Val Acc: 0.7891\n",
            "No improvement. Patience: 9/18\n",
            "Epoch [109/150] Train Loss: 0.6355, Train Acc: 0.5500 | Val Loss: 0.8766, Val Acc: 0.7988\n",
            "No improvement. Patience: 10/18\n",
            "Epoch [110/150] Train Loss: 0.7356, Train Acc: 0.6061 | Val Loss: 0.9166, Val Acc: 0.7855\n",
            "No improvement. Patience: 11/18\n",
            "Epoch [111/150] Train Loss: 0.7388, Train Acc: 0.4639 | Val Loss: 0.9362, Val Acc: 0.7891\n",
            "No improvement. Patience: 12/18\n",
            "Epoch [112/150] Train Loss: 0.8254, Train Acc: 0.5318 | Val Loss: 0.8933, Val Acc: 0.7988\n",
            "No improvement. Patience: 13/18\n",
            "Epoch [113/150] Train Loss: 0.8141, Train Acc: 0.5799 | Val Loss: 0.8930, Val Acc: 0.7915\n",
            "No improvement. Patience: 14/18\n",
            "Epoch [114/150] Train Loss: 0.7233, Train Acc: 0.4823 | Val Loss: 0.8929, Val Acc: 0.7988\n",
            "No improvement. Patience: 15/18\n",
            "Epoch [115/150] Train Loss: 0.6643, Train Acc: 0.4795 | Val Loss: 0.9241, Val Acc: 0.7745\n",
            "No improvement. Patience: 16/18\n",
            "Epoch [116/150] Train Loss: 0.7559, Train Acc: 0.4980 | Val Loss: 0.9199, Val Acc: 0.7818\n",
            "No improvement. Patience: 17/18\n",
            "Epoch [117/150] Train Loss: 0.7165, Train Acc: 0.4547 | Val Loss: 0.9033, Val Acc: 0.7915\n",
            "No improvement. Patience: 18/18\n",
            "\n",
            "Early stopping triggered at epoch 117\n",
            "Best validation accuracy: 0.7988\n",
            "\n",
            "==================================================\n",
            "Training Complete!\n",
            "Best Validation Accuracy: 0.7988\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 150\n",
        "best_val_acc = 0.0\n",
        "patience = 18\n",
        "patience_counter = 0\n",
        "\n",
        "def mixup_data(x, y, alpha=0.15):\n",
        "    lam = np.random.beta(alpha, alpha) if alpha > 0 else 1\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  model.train()\n",
        "  running_train_loss = 0.0\n",
        "  correct_train = 0\n",
        "  total_train = 0\n",
        "\n",
        "  #------ Training Loop -----#\n",
        "  for i, (images, labels) in enumerate(train_loader):\n",
        "    images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    images, labels_a, labels_b, lam = mixup_data(images, labels, alpha=0.15)\n",
        "    outputs = model(images)\n",
        "    loss = lam * criterion(outputs, labels_a) + (1 - lam) * criterion(outputs, labels_b)\n",
        "    loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
        "\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    running_train_loss += loss.item() * images.size(0)\n",
        "    _, predicted = outputs.max(1)\n",
        "    total_train += labels.size(0)\n",
        "    correct_train += predicted.eq(labels).sum().item()\n",
        "\n",
        "  train_loss = running_train_loss / len(train_loader.dataset)\n",
        "  train_acc = correct_train / total_train\n",
        "\n",
        "  # --- Validation Loop ---\n",
        "  model.eval()\n",
        "  running_val_loss = 0.0\n",
        "  correct_val = 0\n",
        "  total_val = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for images, labels in val_loader:\n",
        "          images, labels = images.to(device), labels.to(device)\n",
        "          outputs = model(images)\n",
        "          loss = criterion(outputs, labels)\n",
        "\n",
        "          running_val_loss += loss.item() * images.size(0)\n",
        "          _, predicted = outputs.max(1)\n",
        "          total_val += labels.size(0)\n",
        "          correct_val += predicted.eq(labels).sum().item()\n",
        "\n",
        "  val_loss = running_val_loss / len(val_loader.dataset)\n",
        "  val_acc = correct_val / total_val\n",
        "\n",
        "  print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
        "        f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
        "        f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "\n",
        "  # Save best model and check early stopping\n",
        "  if val_acc > best_val_acc:\n",
        "      best_val_acc = val_acc\n",
        "      patience_counter = 0\n",
        "      torch.save(model.state_dict(), \"model.pth\")\n",
        "      print(f\"‚úì Saved Best Model! Best Val Acc: {best_val_acc:.4f}\")\n",
        "  else:\n",
        "      patience_counter += 1\n",
        "      print(f\"No improvement. Patience: {patience_counter}/{patience}\")\n",
        "\n",
        "      if patience_counter >= patience:\n",
        "          print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n",
        "          print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
        "          break\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"Training Complete!\")\n",
        "print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")\n",
        "print(f\"{'='*50}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "#=== Model Loading Function ===#\n",
        "\n",
        "def load_model(path=\"model.pth\", num_classes=15):\n",
        "    \"\"\"\n",
        "    Load PyTorch model weights from a file.\n",
        "    Returns the model on the specified device.\n",
        "    \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    \n",
        "    # Create model with same architecture as trained\n",
        "    model = theBestCNN(num_classes=num_classes)\n",
        "    \n",
        "    # Load the saved weights\n",
        "    model.load_state_dict(torch.load(path, map_location=device))\n",
        "    \n",
        "    # Move to appropriate device and set to evaluation mode\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    print(f\"Model loaded from {path}\")\n",
        "    return model\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded from model.pth\n",
            "‚úÖ Input shape: torch.Size([64, 3, 64, 64])\n",
            "‚úÖ Output shape: torch.Size([64])\n",
            "‚úÖ Sample predictions: tensor([11, 11, 11, 11, 11], device='cuda:0')\n",
            "‚úÖ Accuracy check: 0.8281\n",
            "üéâ Predict function works!\n"
          ]
        }
      ],
      "source": [
        "def predict(model, images):\n",
        "    \"\"\"\n",
        "    Predict classes for a batch of images.\n",
        "    \n",
        "    Args:\n",
        "        model: the trained PyTorch model\n",
        "        images: torch.Tensor of shape (N, C, H, W)\n",
        "    Returns:\n",
        "        preds: torch.Tensor of predicted class indices (N,)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    images = images.to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(images)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "    return predictions\n",
        "\n",
        "# Test it correctly:\n",
        "test_model = load_model(\"model.pth\")\n",
        "\n",
        "# Get a batch from the DataLoader\n",
        "test_images, test_labels = next(iter(val_loader))\n",
        "est_images = test_images.to(device)  # Move to GPU\n",
        "test_labels = test_labels.to(device)  # Move labels to GPU too!\n",
        "\n",
        "# Pass the IMAGES tensor, not the DataLoader\n",
        "preds = predict(test_model, test_images)\n",
        "\n",
        "print(f\"‚úÖ Input shape: {test_images.shape}\")\n",
        "print(f\"‚úÖ Output shape: {preds.shape}\")\n",
        "print(f\"‚úÖ Sample predictions: {preds[:5]}\")\n",
        "print(f\"‚úÖ Accuracy check: {(preds == test_labels).float().mean():.4f}\")\n",
        "print(\"üéâ Predict function works!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded from model.pth\n",
            "üß™ COMPREHENSIVE MODEL DIAGNOSTICS\n",
            "==================================================\n",
            "1. Full Validation Accuracy: 0.8158 (673/825)\n",
            "\n",
            "2. Per-Class Accuracy:\n",
            "   Class  0: 0.909 (50/55)\n",
            "   Class  1: 0.891 (49/55)\n",
            "   Class  2: 0.727 (40/55)\n",
            "   Class  3: 0.855 (47/55)\n",
            "   Class  4: 0.818 (45/55)\n",
            "   Class  5: 0.818 (45/55)\n",
            "   Class  6: 0.891 (49/55)\n",
            "   Class  7: 0.764 (42/55)\n",
            "   Class  8: 0.891 (49/55)\n",
            "   Class  9: 0.709 (39/55)\n",
            "   Class 10: 0.800 (44/55)\n",
            "   Class 11: 0.800 (44/55)\n",
            "   Class 12: 0.818 (45/55)\n",
            "   Class 13: 0.891 (49/55)\n",
            "   Class 14: 0.655 (36/55)\n",
            "\n",
            "3. Prediction Diversity: 15/15 classes predicted\n",
            "\n",
            "4. Batch Consistency Test:\n",
            "   Batch 0: 9 unique classes predicted\n",
            "üîç Let's examine the actual class distribution in batches:\n",
            "Batch 0: 2 actual classes in the batch\n",
            "Batch 1: 2 actual classes in the batch\n",
            "Batch 2: 2 actual classes in the batch\n",
            "   Batch 1: 9 unique classes predicted\n",
            "üîç Let's examine the actual class distribution in batches:\n",
            "Batch 0: 2 actual classes in the batch\n",
            "Batch 1: 2 actual classes in the batch\n",
            "Batch 2: 2 actual classes in the batch\n",
            "   Batch 2: 9 unique classes predicted\n",
            "üîç Let's examine the actual class distribution in batches:\n",
            "Batch 0: 2 actual classes in the batch\n",
            "Batch 1: 2 actual classes in the batch\n",
            "Batch 2: 2 actual classes in the batch\n",
            "\n",
            "5. FINAL ASSESSMENT:\n",
            "   ‚úÖ EXCELLENT - High chance of test set success!\n",
            "   Model is balanced, accurate, and predicts diverse classes\n"
          ]
        }
      ],
      "source": [
        "# === FIXED ULTIMATE MODEL HEALTH CHECK ===\n",
        "\n",
        "def comprehensive_model_test(model_path=\"model.pth\"):\n",
        "    model = load_model(model_path)\n",
        "    model.eval()\n",
        "    \n",
        "    print(\"üß™ COMPREHENSIVE MODEL DIAGNOSTICS\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # 1. Full validation accuracy\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            # Move everything to GPU\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            preds = predict(model, images)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "            all_predictions.extend(preds.cpu().numpy())  # Move to CPU for storage\n",
        "            all_labels.extend(labels.cpu().numpy())      # Move to CPU for storage\n",
        "    \n",
        "    val_acc = correct / total\n",
        "    print(f\"1. Full Validation Accuracy: {val_acc:.4f} ({correct}/{total})\")\n",
        "    \n",
        "    # 2. Per-class accuracy (critical!)\n",
        "    from collections import Counter\n",
        "    class_correct = [0] * 15\n",
        "    class_total = [0] * 15\n",
        "    \n",
        "    for pred, label in zip(all_predictions, all_labels):\n",
        "        class_total[label] += 1\n",
        "        if pred == label:\n",
        "            class_correct[label] += 1\n",
        "    \n",
        "    print(\"\\n2. Per-Class Accuracy:\")\n",
        "    weak_classes = []\n",
        "    for i in range(15):\n",
        "        acc = class_correct[i] / class_total[i] if class_total[i] > 0 else 0\n",
        "        print(f\"   Class {i:2d}: {acc:.3f} ({class_correct[i]}/{class_total[i]})\")\n",
        "        if acc < 0.6:  # Classes under 60% are concerning\n",
        "            weak_classes.append(i)\n",
        "    \n",
        "    # 3. Prediction diversity test\n",
        "    unique_preds = len(Counter(all_predictions))\n",
        "    print(f\"\\n3. Prediction Diversity: {unique_preds}/15 classes predicted\")\n",
        "    \n",
        "    # 4. Batch-wise consistency test\n",
        "    print(\"\\n4. Batch Consistency Test:\")\n",
        "    for i in range(3):\n",
        "        test_images, test_labels = next(iter(val_loader))\n",
        "        test_images = test_images.to(device)\n",
        "        test_labels = test_labels.to(device)\n",
        "        preds = predict(model, test_images)\n",
        "        unique_in_batch = len(torch.unique(preds))\n",
        "        print(f\"   Batch {i}: {unique_in_batch} unique classes predicted\")\n",
        "\n",
        "        # Let's see what's actually in the batches\n",
        "        print(\"üîç Let's examine the actual class distribution in batches:\")\n",
        "\n",
        "        for i in range(3):\n",
        "            test_images, test_labels = next(iter(val_loader))\n",
        "            unique_actual_classes = len(torch.unique(test_labels))\n",
        "            print(f\"Batch {i}: {unique_actual_classes} actual classes in the batch\")\n",
        "            \n",
        "    # 5. Final Assessment\n",
        "    print(\"\\n5. FINAL ASSESSMENT:\")\n",
        "    if val_acc >= 0.78 and unique_preds >= 12 and len(weak_classes) <= 3:\n",
        "        print(\"   ‚úÖ EXCELLENT - High chance of test set success!\")\n",
        "        print(\"   Model is balanced, accurate, and predicts diverse classes\")\n",
        "    elif val_acc >= 0.75 and unique_preds >= 10:\n",
        "        print(\"   ‚úÖ GOOD - Competitive model\")\n",
        "        print(\"   Should perform well on test set\")\n",
        "    else:\n",
        "        print(\"   ‚ö†Ô∏è  NEEDS IMPROVEMENT - Test set performance uncertain\")\n",
        "        if weak_classes:\n",
        "            print(f\"   Weak classes: {weak_classes}\")\n",
        "        if unique_preds < 10:\n",
        "            print(f\"   Only predicting {unique_preds}/15 classes\")\n",
        "    \n",
        "    return val_acc, weak_classes\n",
        "\n",
        "# Run the comprehensive test\n",
        "final_accuracy, weak_classes = comprehensive_model_test()\n",
        "    "
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
