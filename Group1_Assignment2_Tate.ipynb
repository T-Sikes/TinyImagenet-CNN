{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoOpktIFdWdf"
      },
      "source": [
        "# Data Processing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EP8wLB-04_GT",
        "outputId": "7e86a1fb-a83b-4c72-e860-635ba486a556"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "#reproducability\n",
        "import random, os\n",
        "import numpy as np\n",
        "import torch\n",
        "import pickle\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Device:\" , device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Mo2yRKeTGIS2",
        "outputId": "b87fdafa-4bae-4702-b2a5-aba8d60c85f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train images shape: (5775, 64, 64, 3)\n",
            "Train lables shape: 5775\n",
            "Number of classes: 15\n",
            "class names: [6, 22, 26, 28, 35, 57, 62, 70, 108, 139, 151, 163, 173, 188, 189]\n",
            "Val images shape: (825, 64, 64, 3)\n",
            "val labels shape: 825\n",
            "Number of classes: 15\n"
          ]
        }
      ],
      "source": [
        "\n",
        "train_path = os.path.join('Data', 'train-70_.pkl')\n",
        "val_path   = os.path.join('Data', 'validation-10_.pkl')\n",
        "\n",
        "#----Load training data-----#\n",
        "with open(train_path, 'rb') as f:\n",
        "    train_data = pickle.load(f)\n",
        "\n",
        "images_train = train_data['images']\n",
        "labels_train = train_data['labels']\n",
        "all_classes = sorted(list(set(labels_train)))  # 15 classes\n",
        "\n",
        "\n",
        "print(\"Train images shape:\", images_train.shape)\n",
        "print(\"Train lables shape:\", len(labels_train))\n",
        "print(\"Number of classes:\", len(all_classes))\n",
        "print(\"class names:\", all_classes)\n",
        "\n",
        "\n",
        "with open(val_path, 'rb') as f:\n",
        "    val_data = pickle.load(f)\n",
        "\n",
        "images_val = val_data['images']\n",
        "labels_val = val_data['labels']\n",
        "\n",
        "print(\"Val images shape:\", images_val.shape)\n",
        "print(\"val labels shape:\", len(labels_val))\n",
        "print(\"Number of classes:\", len(all_classes))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NPP4_gz4IyYZ",
        "outputId": "610006a8-7fbb-425c-9233-da433d80f976"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mapped train labels range: tensor(0) tensor(14)\n",
            "Mapped val labels range: tensor(0) tensor(14)\n"
          ]
        }
      ],
      "source": [
        "#convert data to pyTorch tensors for manipulation and modeling\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# ---- Map original labels to 0..num_classes-1 ----\n",
        "class_to_idx = {cls: i for i, cls in enumerate(all_classes)}\n",
        "\n",
        "train_labels_mapped = torch.tensor([class_to_idx[l] for l in labels_train]).long()\n",
        "val_labels_mapped   = torch.tensor([class_to_idx[l] for l in labels_val]).long()\n",
        "\n",
        "# Now you can use these in your dataset\n",
        "train_labels_tensor = train_labels_mapped\n",
        "val_labels_tensor   = val_labels_mapped\n",
        "\n",
        "print(\"Mapped train labels range:\", train_labels_tensor.min(), train_labels_tensor.max())\n",
        "print(\"Mapped val labels range:\", val_labels_tensor.min(), val_labels_tensor.max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Mp239wz_W_0z",
        "outputId": "252cf082-a702-4cb5-fa07-62db8cfc4ae3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Channel-wise mean: [0.47475025 0.4356516  0.38977522]\n",
            "Channel-wise std: [0.26744732 0.2618989  0.2737367 ]\n"
          ]
        }
      ],
      "source": [
        "#standardization of data\n",
        "#uses mean and std for RGB channels across training set\n",
        "train_images_float = images_train.astype(np.float32) / 255.0\n",
        "\n",
        "mean = train_images_float.mean((0,1,2))\n",
        "std = train_images_float.std((0,1,2))\n",
        "\n",
        "print(\"Channel-wise mean:\", mean)\n",
        "print(\"Channel-wise std:\", std)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oW_Rb_tHY_ub"
      },
      "outputs": [],
      "source": [
        "#data set class to wrap tensor\n",
        "#applies image augmentations\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.RandomCrop(64, padding = 4),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(8),\n",
        "   \n",
        "    transforms.ColorJitter(\n",
        "        brightness=0.15, \n",
        "        contrast=0.15, \n",
        "        saturation=0.15, \n",
        "        hue=0.03\n",
        "    ),\n",
        "    #transforms.RandomAffine(\n",
        "       # degrees=0,\n",
        "      #  translate=(0.15,0.15),\n",
        "     #   scale=(0.85, 1.15)\n",
        "    #),\n",
        "    transforms.RandomGrayscale(p=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean.tolist(), std.tolist())\n",
        "])\n",
        "\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean.tolist(), std=std.tolist())\n",
        "])\n",
        "\n",
        "class TinyImageNetDataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Create datasets with transforms\n",
        "train_dataset = TinyImageNetDataset(images_train, train_labels_tensor, train_transform)\n",
        "val_dataset = TinyImageNetDataset(images_val, val_labels_tensor, val_transform)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OO3KXd7UdOep",
        "outputId": "121f9d34-c665-4b54-cd8c-e54aa73cf7e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train batch images shape: torch.Size([64, 3, 64, 64])\n",
            "Train batch labels shape: torch.Size([64])\n"
          ]
        }
      ],
      "source": [
        "# Data Loaders\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = True, #training data must be shuffled\n",
        "    num_workers = 0, #number of parallel data loaders\n",
        "    pin_memory = True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = False, #validation data does not need shuffing\n",
        "    num_workers = 0,\n",
        "    pin_memory = True\n",
        ")\n",
        "\n",
        "for images, labels in train_loader:\n",
        "    print(\"Train batch images shape:\", images.shape)\n",
        "    print(\"Train batch labels shape:\", labels.shape)\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3g8xUhbY9GX"
      },
      "source": [
        "# Model Helpers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "szWc3og3ZVpG"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim import AdamW\n",
        "import numpy as np\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, SequentialLR, LinearLR, OneCycleLR, CosineAnnealingLR\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HrahOlxkZT0d"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, pool=True, dropout=0.0, kernel_size=3):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size=kernel_size,\n",
        "            padding=kernel_size // 2  # keeps spatial size same\n",
        "        )\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.dropout = nn.Dropout2d(dropout) if dropout > 0 else nn.Identity()\n",
        "        self.pool = nn.MaxPool2d(2, 2) if pool else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.pool(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-O3vQ0AZBk3"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Ygf92rmPc4VC"
      },
      "outputs": [],
      "source": [
        "class theBestCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(theBestCNN, self).__init__()\n",
        "\n",
        "        # --- Convolutional blocks ---\n",
        "        self.conv1 = ConvBlock(3, 128, dropout=0.0, kernel_size=3) \n",
        "        self.conv2 = ConvBlock(128, 128, dropout=0.05, kernel_size=3, pool=False) \n",
        "\n",
        "        self.conv3 = ConvBlock(128, 256, dropout=0.1) \n",
        "        self.conv4 = ConvBlock(256, 256, dropout=0.1, pool=False) \n",
        "\n",
        "        self.conv5 = ConvBlock(256, 512, dropout=0.15) \n",
        "        self.conv6 = ConvBlock(512, 512, dropout=0.15, pool=False) \n",
        "\n",
        "        self.conv7 = ConvBlock(512, 1024, dropout = 0.2)  \n",
        "        self.conv8 = ConvBlock(1024, 1024, dropout = 0.2, pool=False) \n",
        "\n",
        "        # --- Global Average Pooling --#\n",
        "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "      \n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512,256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256,num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.conv5(x)\n",
        "        x = self.conv6(x)\n",
        "        x = self.conv7(x)\n",
        "        x = self.conv8(x)\n",
        "\n",
        "        x = self.gap(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "theBestCNN(\n",
            "  (conv1): ConvBlock(\n",
            "    (conv): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (dropout): Identity()\n",
            "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv2): ConvBlock(\n",
            "    (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (dropout): Dropout2d(p=0.05, inplace=False)\n",
            "    (pool): Identity()\n",
            "  )\n",
            "  (conv3): ConvBlock(\n",
            "    (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
            "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv4): ConvBlock(\n",
            "    (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
            "    (pool): Identity()\n",
            "  )\n",
            "  (conv5): ConvBlock(\n",
            "    (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (dropout): Dropout2d(p=0.15, inplace=False)\n",
            "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv6): ConvBlock(\n",
            "    (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (dropout): Dropout2d(p=0.15, inplace=False)\n",
            "    (pool): Identity()\n",
            "  )\n",
            "  (conv7): ConvBlock(\n",
            "    (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (dropout): Dropout2d(p=0.2, inplace=False)\n",
            "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv8): ConvBlock(\n",
            "    (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (dropout): Dropout2d(p=0.2, inplace=False)\n",
            "    (pool): Identity()\n",
            "  )\n",
            "  (gap): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Dropout(p=0.3, inplace=False)\n",
            "    (4): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Dropout(p=0.2, inplace=False)\n",
            "    (8): Linear(in_features=256, out_features=15, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "#instantiate model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = theBestCNN(num_classes=len(all_classes)).to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "4Rn24lUggJLX"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Loss Function ---\n",
        "#loss function\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.04)\n",
        "\n",
        "#optimizer with l2 regularization (weight decay)\n",
        "optimizer = torch.optim.SGD(\n",
        "    model.parameters(),\n",
        "    lr=0.121,\n",
        "    momentum=0.9,\n",
        "    weight_decay=1.5e-4,\n",
        "    nesterov=True\n",
        ")\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=0.121,\n",
        "    epochs=150,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    pct_start=0.1,\n",
        "    anneal_strategy='cos',\n",
        "    div_factor=25.0,\n",
        "    final_div_factor=10000.0\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "collapsed": true,
        "id": "VVaF0bv2geY8",
        "outputId": "0aa8515f-3b6a-44e6-f508-963801c6bfc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/150] Train Loss: 2.7078, Train Acc: 0.0852 | Val Loss: 2.4913, Val Acc: 0.2073\n",
            "âœ“ Saved Best Model! Best Val Acc: 0.2073\n",
            "Epoch [2/150] Train Loss: 2.5311, Train Acc: 0.1231 | Val Loss: 2.3091, Val Acc: 0.2752\n",
            "âœ“ Saved Best Model! Best Val Acc: 0.2752\n",
            "Epoch [3/150] Train Loss: 2.3883, Train Acc: 0.1683 | Val Loss: 2.0386, Val Acc: 0.3709\n",
            "âœ“ Saved Best Model! Best Val Acc: 0.3709\n",
            "Epoch [4/150] Train Loss: 2.2317, Train Acc: 0.1829 | Val Loss: 2.1138, Val Acc: 0.3418\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [5/150] Train Loss: 2.2100, Train Acc: 0.2062 | Val Loss: 2.0312, Val Acc: 0.3345\n",
            "No improvement. Patience: 2/18\n",
            "Epoch [6/150] Train Loss: 2.1331, Train Acc: 0.2256 | Val Loss: 1.8414, Val Acc: 0.4315\n",
            "âœ“ Saved Best Model! Best Val Acc: 0.4315\n",
            "Epoch [7/150] Train Loss: 2.1222, Train Acc: 0.2675 | Val Loss: 2.0427, Val Acc: 0.3648\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [8/150] Train Loss: 2.0548, Train Acc: 0.2331 | Val Loss: 1.9458, Val Acc: 0.3964\n",
            "No improvement. Patience: 2/18\n",
            "Epoch [9/150] Train Loss: 2.0473, Train Acc: 0.2255 | Val Loss: 1.7593, Val Acc: 0.4691\n",
            "âœ“ Saved Best Model! Best Val Acc: 0.4691\n",
            "Epoch [10/150] Train Loss: 2.0249, Train Acc: 0.2417 | Val Loss: 1.9875, Val Acc: 0.4182\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [11/150] Train Loss: 1.9886, Train Acc: 0.2604 | Val Loss: 1.9065, Val Acc: 0.4230\n",
            "No improvement. Patience: 2/18\n",
            "Epoch [12/150] Train Loss: 1.9448, Train Acc: 0.2649 | Val Loss: 1.7100, Val Acc: 0.4933\n",
            "âœ“ Saved Best Model! Best Val Acc: 0.4933\n",
            "Epoch [13/150] Train Loss: 1.9298, Train Acc: 0.2952 | Val Loss: 1.6998, Val Acc: 0.4994\n",
            "âœ“ Saved Best Model! Best Val Acc: 0.4994\n",
            "Epoch [14/150] Train Loss: 1.8493, Train Acc: 0.3106 | Val Loss: 1.7757, Val Acc: 0.4727\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [15/150] Train Loss: 1.8418, Train Acc: 0.2913 | Val Loss: 1.5692, Val Acc: 0.5261\n",
            "âœ“ Saved Best Model! Best Val Acc: 0.5261\n",
            "Epoch [16/150] Train Loss: 1.8344, Train Acc: 0.3158 | Val Loss: 1.6590, Val Acc: 0.5103\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [17/150] Train Loss: 1.7741, Train Acc: 0.3049 | Val Loss: 1.6114, Val Acc: 0.5224\n",
            "No improvement. Patience: 2/18\n",
            "Epoch [18/150] Train Loss: 1.7835, Train Acc: 0.2914 | Val Loss: 1.4652, Val Acc: 0.5588\n",
            "âœ“ Saved Best Model! Best Val Acc: 0.5588\n",
            "Epoch [19/150] Train Loss: 1.7100, Train Acc: 0.2975 | Val Loss: 1.4570, Val Acc: 0.5745\n",
            "âœ“ Saved Best Model! Best Val Acc: 0.5745\n",
            "Epoch [20/150] Train Loss: 1.6630, Train Acc: 0.3048 | Val Loss: 1.4748, Val Acc: 0.5697\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [21/150] Train Loss: 1.6927, Train Acc: 0.3425 | Val Loss: 1.6091, Val Acc: 0.5164\n",
            "No improvement. Patience: 2/18\n",
            "Epoch [22/150] Train Loss: 1.7187, Train Acc: 0.3261 | Val Loss: 1.4357, Val Acc: 0.5939\n",
            "âœ“ Saved Best Model! Best Val Acc: 0.5939\n",
            "Epoch [23/150] Train Loss: 1.6371, Train Acc: 0.3269 | Val Loss: 1.4129, Val Acc: 0.5927\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [24/150] Train Loss: 1.5530, Train Acc: 0.3758 | Val Loss: 1.3271, Val Acc: 0.6327\n",
            "âœ“ Saved Best Model! Best Val Acc: 0.6327\n",
            "Epoch [25/150] Train Loss: 1.5755, Train Acc: 0.3169 | Val Loss: 1.3952, Val Acc: 0.5867\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [26/150] Train Loss: 1.6557, Train Acc: 0.3290 | Val Loss: 1.4493, Val Acc: 0.5794\n",
            "No improvement. Patience: 2/18\n",
            "Epoch [27/150] Train Loss: 1.5466, Train Acc: 0.3628 | Val Loss: 1.4725, Val Acc: 0.5770\n",
            "No improvement. Patience: 3/18\n",
            "Epoch [28/150] Train Loss: 1.5325, Train Acc: 0.3392 | Val Loss: 1.3076, Val Acc: 0.6315\n",
            "No improvement. Patience: 4/18\n",
            "Epoch [29/150] Train Loss: 1.5202, Train Acc: 0.3349 | Val Loss: 1.2629, Val Acc: 0.6327\n",
            "No improvement. Patience: 5/18\n",
            "Epoch [30/150] Train Loss: 1.4964, Train Acc: 0.3467 | Val Loss: 1.2831, Val Acc: 0.6315\n",
            "No improvement. Patience: 6/18\n",
            "Epoch [31/150] Train Loss: 1.4836, Train Acc: 0.3818 | Val Loss: 1.2215, Val Acc: 0.6533\n",
            "âœ“ Saved Best Model! Best Val Acc: 0.6533\n",
            "Epoch [32/150] Train Loss: 1.4817, Train Acc: 0.3467 | Val Loss: 1.2968, Val Acc: 0.6352\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [33/150] Train Loss: 1.5023, Train Acc: 0.3522 | Val Loss: 1.2165, Val Acc: 0.6618\n",
            "âœ“ Saved Best Model! Best Val Acc: 0.6618\n",
            "Epoch [34/150] Train Loss: 1.4036, Train Acc: 0.3754 | Val Loss: 1.1787, Val Acc: 0.6848\n",
            "âœ“ Saved Best Model! Best Val Acc: 0.6848\n",
            "Epoch [35/150] Train Loss: 1.4336, Train Acc: 0.3640 | Val Loss: 1.3359, Val Acc: 0.6400\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [36/150] Train Loss: 1.4472, Train Acc: 0.3657 | Val Loss: 1.1598, Val Acc: 0.7006\n",
            "âœ“ Saved Best Model! Best Val Acc: 0.7006\n",
            "Epoch [37/150] Train Loss: 1.3722, Train Acc: 0.3745 | Val Loss: 1.1723, Val Acc: 0.6752\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [38/150] Train Loss: 1.3235, Train Acc: 0.3702 | Val Loss: 1.1685, Val Acc: 0.6739\n",
            "No improvement. Patience: 2/18\n",
            "Epoch [39/150] Train Loss: 1.3186, Train Acc: 0.4338 | Val Loss: 1.1180, Val Acc: 0.7042\n",
            "âœ“ Saved Best Model! Best Val Acc: 0.7042\n",
            "Epoch [40/150] Train Loss: 1.3490, Train Acc: 0.3210 | Val Loss: 1.1573, Val Acc: 0.6836\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [41/150] Train Loss: 1.3455, Train Acc: 0.3792 | Val Loss: 1.2183, Val Acc: 0.6618\n",
            "No improvement. Patience: 2/18\n",
            "Epoch [42/150] Train Loss: 1.3050, Train Acc: 0.4317 | Val Loss: 1.1318, Val Acc: 0.6836\n",
            "No improvement. Patience: 3/18\n",
            "Epoch [43/150] Train Loss: 1.2931, Train Acc: 0.3446 | Val Loss: 1.1295, Val Acc: 0.7091\n",
            "âœ“ Saved Best Model! Best Val Acc: 0.7091\n",
            "Epoch [44/150] Train Loss: 1.2628, Train Acc: 0.4447 | Val Loss: 1.1810, Val Acc: 0.6812\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [45/150] Train Loss: 1.2456, Train Acc: 0.4237 | Val Loss: 1.0879, Val Acc: 0.7127\n",
            "âœ“ Saved Best Model! Best Val Acc: 0.7127\n",
            "Epoch [46/150] Train Loss: 1.3286, Train Acc: 0.3910 | Val Loss: 1.0546, Val Acc: 0.7273\n",
            "âœ“ Saved Best Model! Best Val Acc: 0.7273\n",
            "Epoch [47/150] Train Loss: 1.2891, Train Acc: 0.3915 | Val Loss: 1.1230, Val Acc: 0.7042\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [48/150] Train Loss: 1.2817, Train Acc: 0.3744 | Val Loss: 1.0228, Val Acc: 0.7358\n",
            "âœ“ Saved Best Model! Best Val Acc: 0.7358\n",
            "Epoch [49/150] Train Loss: 1.2377, Train Acc: 0.3886 | Val Loss: 1.1530, Val Acc: 0.6921\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [50/150] Train Loss: 1.1897, Train Acc: 0.4177 | Val Loss: 1.0584, Val Acc: 0.7115\n",
            "No improvement. Patience: 2/18\n",
            "Epoch [51/150] Train Loss: 1.1363, Train Acc: 0.4248 | Val Loss: 1.0675, Val Acc: 0.7224\n",
            "No improvement. Patience: 3/18\n",
            "Epoch [52/150] Train Loss: 1.1551, Train Acc: 0.4767 | Val Loss: 1.0728, Val Acc: 0.7297\n",
            "No improvement. Patience: 4/18\n",
            "Epoch [53/150] Train Loss: 1.1768, Train Acc: 0.4000 | Val Loss: 1.0267, Val Acc: 0.7527\n",
            "âœ“ Saved Best Model! Best Val Acc: 0.7527\n",
            "Epoch [54/150] Train Loss: 1.1959, Train Acc: 0.4452 | Val Loss: 1.0580, Val Acc: 0.7406\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [55/150] Train Loss: 1.0948, Train Acc: 0.4686 | Val Loss: 1.0575, Val Acc: 0.7188\n",
            "No improvement. Patience: 2/18\n",
            "Epoch [56/150] Train Loss: 1.1452, Train Acc: 0.5001 | Val Loss: 0.9956, Val Acc: 0.7406\n",
            "No improvement. Patience: 3/18\n",
            "Epoch [57/150] Train Loss: 1.2153, Train Acc: 0.4104 | Val Loss: 1.0542, Val Acc: 0.7418\n",
            "No improvement. Patience: 4/18\n",
            "Epoch [58/150] Train Loss: 1.0742, Train Acc: 0.4452 | Val Loss: 0.9869, Val Acc: 0.7467\n",
            "No improvement. Patience: 5/18\n",
            "Epoch [59/150] Train Loss: 1.1136, Train Acc: 0.4445 | Val Loss: 1.0254, Val Acc: 0.7370\n",
            "No improvement. Patience: 6/18\n",
            "Epoch [60/150] Train Loss: 1.0257, Train Acc: 0.3749 | Val Loss: 1.0461, Val Acc: 0.7455\n",
            "No improvement. Patience: 7/18\n",
            "Epoch [61/150] Train Loss: 1.0828, Train Acc: 0.4409 | Val Loss: 1.0200, Val Acc: 0.7455\n",
            "No improvement. Patience: 8/18\n",
            "Epoch [62/150] Train Loss: 1.0594, Train Acc: 0.3841 | Val Loss: 1.0151, Val Acc: 0.7455\n",
            "No improvement. Patience: 9/18\n",
            "Epoch [63/150] Train Loss: 1.1632, Train Acc: 0.4708 | Val Loss: 0.9808, Val Acc: 0.7697\n",
            "âœ“ Saved Best Model! Best Val Acc: 0.7697\n",
            "Epoch [64/150] Train Loss: 1.0741, Train Acc: 0.4113 | Val Loss: 0.9916, Val Acc: 0.7612\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [65/150] Train Loss: 1.0460, Train Acc: 0.4277 | Val Loss: 0.9933, Val Acc: 0.7539\n",
            "No improvement. Patience: 2/18\n",
            "Epoch [66/150] Train Loss: 1.0990, Train Acc: 0.4126 | Val Loss: 0.9762, Val Acc: 0.7624\n",
            "No improvement. Patience: 3/18\n",
            "Epoch [67/150] Train Loss: 1.0202, Train Acc: 0.4211 | Val Loss: 0.9851, Val Acc: 0.7382\n",
            "No improvement. Patience: 4/18\n",
            "Epoch [68/150] Train Loss: 1.0511, Train Acc: 0.4906 | Val Loss: 0.9780, Val Acc: 0.7588\n",
            "No improvement. Patience: 5/18\n",
            "Epoch [69/150] Train Loss: 1.0231, Train Acc: 0.4346 | Val Loss: 0.9830, Val Acc: 0.7527\n",
            "No improvement. Patience: 6/18\n",
            "Epoch [70/150] Train Loss: 1.0043, Train Acc: 0.4495 | Val Loss: 0.9672, Val Acc: 0.7661\n",
            "No improvement. Patience: 7/18\n",
            "Epoch [71/150] Train Loss: 0.9643, Train Acc: 0.4511 | Val Loss: 0.9648, Val Acc: 0.7673\n",
            "No improvement. Patience: 8/18\n",
            "Epoch [72/150] Train Loss: 0.9164, Train Acc: 0.5044 | Val Loss: 0.9284, Val Acc: 0.7733\n",
            "âœ“ Saved Best Model! Best Val Acc: 0.7733\n",
            "Epoch [73/150] Train Loss: 1.0146, Train Acc: 0.5229 | Val Loss: 0.9741, Val Acc: 0.7600\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [74/150] Train Loss: 0.9726, Train Acc: 0.5572 | Val Loss: 0.9993, Val Acc: 0.7455\n",
            "No improvement. Patience: 2/18\n",
            "Epoch [75/150] Train Loss: 0.9518, Train Acc: 0.4061 | Val Loss: 1.0112, Val Acc: 0.7418\n",
            "No improvement. Patience: 3/18\n",
            "Epoch [76/150] Train Loss: 0.9082, Train Acc: 0.5203 | Val Loss: 0.9512, Val Acc: 0.7576\n",
            "No improvement. Patience: 4/18\n",
            "Epoch [77/150] Train Loss: 0.9274, Train Acc: 0.4587 | Val Loss: 0.9460, Val Acc: 0.7661\n",
            "No improvement. Patience: 5/18\n",
            "Epoch [78/150] Train Loss: 0.9464, Train Acc: 0.4206 | Val Loss: 0.9701, Val Acc: 0.7600\n",
            "No improvement. Patience: 6/18\n",
            "Epoch [79/150] Train Loss: 0.9430, Train Acc: 0.5155 | Val Loss: 0.9566, Val Acc: 0.7539\n",
            "No improvement. Patience: 7/18\n",
            "Epoch [80/150] Train Loss: 0.9098, Train Acc: 0.4601 | Val Loss: 1.0059, Val Acc: 0.7491\n",
            "No improvement. Patience: 8/18\n",
            "Epoch [81/150] Train Loss: 1.0097, Train Acc: 0.5300 | Val Loss: 0.9298, Val Acc: 0.7709\n",
            "No improvement. Patience: 9/18\n",
            "Epoch [82/150] Train Loss: 0.8751, Train Acc: 0.4459 | Val Loss: 0.9303, Val Acc: 0.7721\n",
            "No improvement. Patience: 10/18\n",
            "Epoch [83/150] Train Loss: 0.8768, Train Acc: 0.4156 | Val Loss: 0.9660, Val Acc: 0.7576\n",
            "No improvement. Patience: 11/18\n",
            "Epoch [84/150] Train Loss: 0.9414, Train Acc: 0.4916 | Val Loss: 0.9834, Val Acc: 0.7503\n",
            "No improvement. Patience: 12/18\n",
            "Epoch [85/150] Train Loss: 0.8528, Train Acc: 0.4478 | Val Loss: 0.9668, Val Acc: 0.7661\n",
            "No improvement. Patience: 13/18\n",
            "Epoch [86/150] Train Loss: 0.8694, Train Acc: 0.5029 | Val Loss: 0.9555, Val Acc: 0.7745\n",
            "âœ“ Saved Best Model! Best Val Acc: 0.7745\n",
            "Epoch [87/150] Train Loss: 0.9630, Train Acc: 0.4987 | Val Loss: 0.9637, Val Acc: 0.7673\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [88/150] Train Loss: 0.8887, Train Acc: 0.4585 | Val Loss: 0.9352, Val Acc: 0.7733\n",
            "No improvement. Patience: 2/18\n",
            "Epoch [89/150] Train Loss: 0.8273, Train Acc: 0.4845 | Val Loss: 0.9442, Val Acc: 0.7782\n",
            "âœ“ Saved Best Model! Best Val Acc: 0.7782\n",
            "Epoch [90/150] Train Loss: 0.7919, Train Acc: 0.4689 | Val Loss: 0.9626, Val Acc: 0.7709\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [91/150] Train Loss: 0.8155, Train Acc: 0.5375 | Val Loss: 0.9569, Val Acc: 0.7636\n",
            "No improvement. Patience: 2/18\n",
            "Epoch [92/150] Train Loss: 0.8075, Train Acc: 0.4661 | Val Loss: 0.9457, Val Acc: 0.7733\n",
            "No improvement. Patience: 3/18\n",
            "Epoch [93/150] Train Loss: 0.9086, Train Acc: 0.3853 | Val Loss: 0.9508, Val Acc: 0.7782\n",
            "No improvement. Patience: 4/18\n",
            "Epoch [94/150] Train Loss: 0.8486, Train Acc: 0.4911 | Val Loss: 0.9546, Val Acc: 0.7709\n",
            "No improvement. Patience: 5/18\n",
            "Epoch [95/150] Train Loss: 0.8197, Train Acc: 0.5094 | Val Loss: 0.9402, Val Acc: 0.7733\n",
            "No improvement. Patience: 6/18\n",
            "Epoch [96/150] Train Loss: 0.8570, Train Acc: 0.4748 | Val Loss: 0.9382, Val Acc: 0.7709\n",
            "No improvement. Patience: 7/18\n",
            "Epoch [97/150] Train Loss: 0.8696, Train Acc: 0.5096 | Val Loss: 0.9352, Val Acc: 0.7855\n",
            "âœ“ Saved Best Model! Best Val Acc: 0.7855\n",
            "Epoch [98/150] Train Loss: 0.9336, Train Acc: 0.4987 | Val Loss: 0.9686, Val Acc: 0.7661\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [99/150] Train Loss: 0.8247, Train Acc: 0.5177 | Val Loss: 0.9574, Val Acc: 0.7697\n",
            "No improvement. Patience: 2/18\n",
            "Epoch [100/150] Train Loss: 0.9067, Train Acc: 0.4279 | Val Loss: 0.9279, Val Acc: 0.7806\n",
            "No improvement. Patience: 3/18\n",
            "Epoch [101/150] Train Loss: 0.8095, Train Acc: 0.4798 | Val Loss: 0.9133, Val Acc: 0.7855\n",
            "No improvement. Patience: 4/18\n",
            "Epoch [102/150] Train Loss: 0.8920, Train Acc: 0.5048 | Val Loss: 0.9229, Val Acc: 0.7915\n",
            "âœ“ Saved Best Model! Best Val Acc: 0.7915\n",
            "Epoch [103/150] Train Loss: 0.7588, Train Acc: 0.5707 | Val Loss: 0.9248, Val Acc: 0.7903\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [104/150] Train Loss: 0.6162, Train Acc: 0.4958 | Val Loss: 0.9326, Val Acc: 0.7806\n",
            "No improvement. Patience: 2/18\n",
            "Epoch [105/150] Train Loss: 0.8814, Train Acc: 0.4781 | Val Loss: 0.9349, Val Acc: 0.7794\n",
            "No improvement. Patience: 3/18\n",
            "Epoch [106/150] Train Loss: 0.7792, Train Acc: 0.4566 | Val Loss: 0.9118, Val Acc: 0.7903\n",
            "No improvement. Patience: 4/18\n",
            "Epoch [107/150] Train Loss: 0.6927, Train Acc: 0.5292 | Val Loss: 0.8979, Val Acc: 0.7867\n",
            "No improvement. Patience: 5/18\n",
            "Epoch [108/150] Train Loss: 0.8720, Train Acc: 0.4840 | Val Loss: 0.9041, Val Acc: 0.7903\n",
            "No improvement. Patience: 6/18\n",
            "Epoch [109/150] Train Loss: 0.7516, Train Acc: 0.5728 | Val Loss: 0.9021, Val Acc: 0.7976\n",
            "âœ“ Saved Best Model! Best Val Acc: 0.7976\n",
            "Epoch [110/150] Train Loss: 0.6714, Train Acc: 0.4490 | Val Loss: 0.9037, Val Acc: 0.7915\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [111/150] Train Loss: 0.9121, Train Acc: 0.5068 | Val Loss: 0.9123, Val Acc: 0.7903\n",
            "No improvement. Patience: 2/18\n",
            "Epoch [112/150] Train Loss: 0.7648, Train Acc: 0.5075 | Val Loss: 0.9154, Val Acc: 0.7830\n",
            "No improvement. Patience: 3/18\n",
            "Epoch [113/150] Train Loss: 0.7338, Train Acc: 0.5432 | Val Loss: 0.8894, Val Acc: 0.8000\n",
            "âœ“ Saved Best Model! Best Val Acc: 0.8000\n",
            "Epoch [114/150] Train Loss: 0.8423, Train Acc: 0.5294 | Val Loss: 0.9011, Val Acc: 0.7794\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [115/150] Train Loss: 0.7253, Train Acc: 0.4809 | Val Loss: 0.8756, Val Acc: 0.8012\n",
            "âœ“ Saved Best Model! Best Val Acc: 0.8012\n",
            "Epoch [116/150] Train Loss: 0.8505, Train Acc: 0.4606 | Val Loss: 0.8854, Val Acc: 0.7964\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [117/150] Train Loss: 0.6353, Train Acc: 0.5539 | Val Loss: 0.9046, Val Acc: 0.7879\n",
            "No improvement. Patience: 2/18\n",
            "Epoch [118/150] Train Loss: 0.8262, Train Acc: 0.5233 | Val Loss: 0.8854, Val Acc: 0.8012\n",
            "No improvement. Patience: 3/18\n",
            "Epoch [119/150] Train Loss: 0.7752, Train Acc: 0.5860 | Val Loss: 0.8977, Val Acc: 0.7988\n",
            "No improvement. Patience: 4/18\n",
            "Epoch [120/150] Train Loss: 0.6511, Train Acc: 0.6151 | Val Loss: 0.8951, Val Acc: 0.7976\n",
            "No improvement. Patience: 5/18\n",
            "Epoch [121/150] Train Loss: 0.7949, Train Acc: 0.5351 | Val Loss: 0.9118, Val Acc: 0.7879\n",
            "No improvement. Patience: 6/18\n",
            "Epoch [122/150] Train Loss: 0.8606, Train Acc: 0.5624 | Val Loss: 0.8900, Val Acc: 0.8109\n",
            "âœ“ Saved Best Model! Best Val Acc: 0.8109\n",
            "Epoch [123/150] Train Loss: 0.7486, Train Acc: 0.5105 | Val Loss: 0.8940, Val Acc: 0.7976\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [124/150] Train Loss: 0.7629, Train Acc: 0.5235 | Val Loss: 0.9049, Val Acc: 0.7806\n",
            "No improvement. Patience: 2/18\n",
            "Epoch [125/150] Train Loss: 0.7250, Train Acc: 0.5262 | Val Loss: 0.8761, Val Acc: 0.8000\n",
            "No improvement. Patience: 3/18\n",
            "Epoch [126/150] Train Loss: 0.7153, Train Acc: 0.4603 | Val Loss: 0.9087, Val Acc: 0.7903\n",
            "No improvement. Patience: 4/18\n",
            "Epoch [127/150] Train Loss: 0.6935, Train Acc: 0.4999 | Val Loss: 0.8819, Val Acc: 0.7903\n",
            "No improvement. Patience: 5/18\n",
            "Epoch [128/150] Train Loss: 0.6327, Train Acc: 0.4772 | Val Loss: 0.8681, Val Acc: 0.7988\n",
            "No improvement. Patience: 6/18\n",
            "Epoch [129/150] Train Loss: 0.6762, Train Acc: 0.5075 | Val Loss: 0.8806, Val Acc: 0.7927\n",
            "No improvement. Patience: 7/18\n",
            "Epoch [130/150] Train Loss: 0.6973, Train Acc: 0.4771 | Val Loss: 0.8853, Val Acc: 0.8000\n",
            "No improvement. Patience: 8/18\n",
            "Epoch [131/150] Train Loss: 0.8936, Train Acc: 0.4345 | Val Loss: 0.8758, Val Acc: 0.7988\n",
            "No improvement. Patience: 9/18\n",
            "Epoch [132/150] Train Loss: 0.6149, Train Acc: 0.5223 | Val Loss: 0.8902, Val Acc: 0.7915\n",
            "No improvement. Patience: 10/18\n",
            "Epoch [133/150] Train Loss: 0.6809, Train Acc: 0.5830 | Val Loss: 0.8710, Val Acc: 0.8036\n",
            "No improvement. Patience: 11/18\n",
            "Epoch [134/150] Train Loss: 0.7179, Train Acc: 0.4511 | Val Loss: 0.8764, Val Acc: 0.7988\n",
            "No improvement. Patience: 12/18\n",
            "Epoch [135/150] Train Loss: 0.6444, Train Acc: 0.4675 | Val Loss: 0.8691, Val Acc: 0.8000\n",
            "No improvement. Patience: 13/18\n",
            "Epoch [136/150] Train Loss: 0.7297, Train Acc: 0.4661 | Val Loss: 0.8801, Val Acc: 0.7988\n",
            "No improvement. Patience: 14/18\n",
            "Epoch [137/150] Train Loss: 0.7198, Train Acc: 0.4996 | Val Loss: 0.8642, Val Acc: 0.8073\n",
            "No improvement. Patience: 15/18\n",
            "Epoch [138/150] Train Loss: 0.6331, Train Acc: 0.4253 | Val Loss: 0.8761, Val Acc: 0.7939\n",
            "No improvement. Patience: 16/18\n",
            "Epoch [139/150] Train Loss: 0.6812, Train Acc: 0.5562 | Val Loss: 0.8625, Val Acc: 0.8133\n",
            "âœ“ Saved Best Model! Best Val Acc: 0.8133\n",
            "Epoch [140/150] Train Loss: 0.6995, Train Acc: 0.4989 | Val Loss: 0.8733, Val Acc: 0.8012\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [141/150] Train Loss: 0.7238, Train Acc: 0.6424 | Val Loss: 0.8618, Val Acc: 0.8097\n",
            "No improvement. Patience: 2/18\n",
            "Epoch [142/150] Train Loss: 0.6926, Train Acc: 0.4078 | Val Loss: 0.8698, Val Acc: 0.8073\n",
            "No improvement. Patience: 3/18\n",
            "Epoch [143/150] Train Loss: 0.7712, Train Acc: 0.5583 | Val Loss: 0.8574, Val Acc: 0.8158\n",
            "âœ“ Saved Best Model! Best Val Acc: 0.8158\n",
            "Epoch [144/150] Train Loss: 0.6371, Train Acc: 0.5579 | Val Loss: 0.8601, Val Acc: 0.8145\n",
            "No improvement. Patience: 1/18\n",
            "Epoch [145/150] Train Loss: 0.6472, Train Acc: 0.5560 | Val Loss: 0.8657, Val Acc: 0.8121\n",
            "No improvement. Patience: 2/18\n",
            "Epoch [146/150] Train Loss: 0.6816, Train Acc: 0.4925 | Val Loss: 0.8590, Val Acc: 0.8085\n",
            "No improvement. Patience: 3/18\n",
            "Epoch [147/150] Train Loss: 0.6591, Train Acc: 0.6057 | Val Loss: 0.8572, Val Acc: 0.8061\n",
            "No improvement. Patience: 4/18\n",
            "Epoch [148/150] Train Loss: 0.5799, Train Acc: 0.6126 | Val Loss: 0.8577, Val Acc: 0.8133\n",
            "No improvement. Patience: 5/18\n",
            "Epoch [149/150] Train Loss: 0.6875, Train Acc: 0.5541 | Val Loss: 0.8710, Val Acc: 0.8000\n",
            "No improvement. Patience: 6/18\n",
            "Epoch [150/150] Train Loss: 0.7004, Train Acc: 0.5406 | Val Loss: 0.8581, Val Acc: 0.8109\n",
            "No improvement. Patience: 7/18\n",
            "\n",
            "==================================================\n",
            "Training Complete!\n",
            "Best Validation Accuracy: 0.8158\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 150\n",
        "best_val_acc = 0.0\n",
        "patience = 18\n",
        "patience_counter = 0\n",
        "\n",
        "def mixup_data(x, y, alpha=0.15):\n",
        "    lam = np.random.beta(alpha, alpha) if alpha > 0 else 1\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  model.train()\n",
        "  running_train_loss = 0.0\n",
        "  correct_train = 0\n",
        "  total_train = 0\n",
        "\n",
        "  #------ Training Loop -----#\n",
        "  for i, (images, labels) in enumerate(train_loader):\n",
        "    images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    images, labels_a, labels_b, lam = mixup_data(images, labels, alpha=0.15)\n",
        "    outputs = model(images)\n",
        "    loss = lam * criterion(outputs, labels_a) + (1 - lam) * criterion(outputs, labels_b)\n",
        "    loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
        "\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    running_train_loss += loss.item() * images.size(0)\n",
        "    _, predicted = outputs.max(1)\n",
        "    total_train += labels.size(0)\n",
        "    correct_train += predicted.eq(labels).sum().item()\n",
        "\n",
        "  train_loss = running_train_loss / len(train_loader.dataset)\n",
        "  train_acc = correct_train / total_train\n",
        "\n",
        "  # --- Validation Loop ---\n",
        "  model.eval()\n",
        "  running_val_loss = 0.0\n",
        "  correct_val = 0\n",
        "  total_val = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for images, labels in val_loader:\n",
        "          images, labels = images.to(device), labels.to(device)\n",
        "          outputs = model(images)\n",
        "          loss = criterion(outputs, labels)\n",
        "\n",
        "          running_val_loss += loss.item() * images.size(0)\n",
        "          _, predicted = outputs.max(1)\n",
        "          total_val += labels.size(0)\n",
        "          correct_val += predicted.eq(labels).sum().item()\n",
        "\n",
        "  val_loss = running_val_loss / len(val_loader.dataset)\n",
        "  val_acc = correct_val / total_val\n",
        "\n",
        "  print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
        "        f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
        "        f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "\n",
        "  # Save best model and check early stopping\n",
        "  if val_acc > best_val_acc:\n",
        "      best_val_acc = val_acc\n",
        "      patience_counter = 0\n",
        "      torch.save(model.state_dict(), \"model.pth\")\n",
        "      print(f\"âœ“ Saved Best Model! Best Val Acc: {best_val_acc:.4f}\")\n",
        "  else:\n",
        "      patience_counter += 1\n",
        "      print(f\"No improvement. Patience: {patience_counter}/{patience}\")\n",
        "\n",
        "      if patience_counter >= patience:\n",
        "          print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n",
        "          print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
        "          break\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"Training Complete!\")\n",
        "print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")\n",
        "print(f\"{'='*50}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CORRECTED VERSION BASED ON ACTUAL REQUIREMENTS\n",
        "\n",
        "def load_model(path=\"model.pth\", num_classes=15):\n",
        "    \"\"\"\n",
        "    Load PyTorch model weights from a file.\n",
        "    Returns the model on the specified device.\n",
        "    \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    \n",
        "    # Create model with same architecture as trained\n",
        "    model = theBestCNN(num_classes=num_classes)\n",
        "    \n",
        "    # Load the saved weights\n",
        "    model.load_state_dict(torch.load(path, map_location=device))\n",
        "    \n",
        "    # Move to appropriate device and set to evaluation mode\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    print(f\"Model loaded from {path}\")\n",
        "    return model\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded from model.pth\n",
            "âœ… Input shape: torch.Size([64, 3, 64, 64])\n",
            "âœ… Output shape: torch.Size([64])\n",
            "âœ… Sample predictions: tensor([11, 11, 11, 11, 11], device='cuda:0')\n",
            "âœ… Accuracy check: 0.8125\n",
            "ðŸŽ‰ Predict function works! Ready for submission.\n"
          ]
        }
      ],
      "source": [
        "def predict(model, images, device=device):\n",
        "    \"\"\"\n",
        "    Predict classes for a batch of images.\n",
        "    \n",
        "    Args:\n",
        "        model: the trained PyTorch model\n",
        "        images: torch.Tensor of shape (N, C, H, W)\n",
        "    Returns:\n",
        "        preds: torch.Tensor of predicted class indices (N,)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    images = images.to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(images)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "    return predictions\n",
        "\n",
        "# Test it correctly:\n",
        "test_model = load_model(\"model.pth\")\n",
        "\n",
        "# Get a batch from the DataLoader\n",
        "test_images, test_labels = next(iter(val_loader))\n",
        "est_images = test_images.to(device)  # Move to GPU\n",
        "test_labels = test_labels.to(device)  # Move labels to GPU too!\n",
        "\n",
        "# Pass the IMAGES tensor, not the DataLoader\n",
        "preds = predict(test_model, test_images)\n",
        "\n",
        "print(f\"âœ… Input shape: {test_images.shape}\")\n",
        "print(f\"âœ… Output shape: {preds.shape}\")\n",
        "print(f\"âœ… Sample predictions: {preds[:5]}\")\n",
        "print(f\"âœ… Accuracy check: {(preds == test_labels).float().mean():.4f}\")\n",
        "print(\"ðŸŽ‰ Predict function works! Ready for submission.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded from model.pth\n",
            "ðŸ§ª COMPREHENSIVE MODEL DIAGNOSTICS\n",
            "==================================================\n",
            "1. Full Validation Accuracy: 0.8158 (673/825)\n",
            "\n",
            "2. Per-Class Accuracy:\n",
            "   Class  0: 0.909 (50/55)\n",
            "   Class  1: 0.891 (49/55)\n",
            "   Class  2: 0.727 (40/55)\n",
            "   Class  3: 0.855 (47/55)\n",
            "   Class  4: 0.818 (45/55)\n",
            "   Class  5: 0.818 (45/55)\n",
            "   Class  6: 0.891 (49/55)\n",
            "   Class  7: 0.764 (42/55)\n",
            "   Class  8: 0.891 (49/55)\n",
            "   Class  9: 0.709 (39/55)\n",
            "   Class 10: 0.800 (44/55)\n",
            "   Class 11: 0.800 (44/55)\n",
            "   Class 12: 0.818 (45/55)\n",
            "   Class 13: 0.891 (49/55)\n",
            "   Class 14: 0.655 (36/55)\n",
            "\n",
            "3. Prediction Diversity: 15/15 classes predicted\n",
            "\n",
            "4. Batch Consistency Test:\n",
            "   Batch 0: 9 unique classes predicted\n",
            "ðŸ” Let's examine the actual class distribution in batches:\n",
            "Batch 0: 2 actual classes in the batch\n",
            "Batch 1: 2 actual classes in the batch\n",
            "Batch 2: 2 actual classes in the batch\n",
            "   Batch 1: 9 unique classes predicted\n",
            "ðŸ” Let's examine the actual class distribution in batches:\n",
            "Batch 0: 2 actual classes in the batch\n",
            "Batch 1: 2 actual classes in the batch\n",
            "Batch 2: 2 actual classes in the batch\n",
            "   Batch 2: 9 unique classes predicted\n",
            "ðŸ” Let's examine the actual class distribution in batches:\n",
            "Batch 0: 2 actual classes in the batch\n",
            "Batch 1: 2 actual classes in the batch\n",
            "Batch 2: 2 actual classes in the batch\n",
            "\n",
            "5. FINAL ASSESSMENT:\n",
            "   âœ… EXCELLENT - High chance of test set success!\n",
            "   Model is balanced, accurate, and predicts diverse classes\n"
          ]
        }
      ],
      "source": [
        "# === FIXED ULTIMATE MODEL HEALTH CHECK ===\n",
        "\n",
        "def comprehensive_model_test(model_path=\"model.pth\"):\n",
        "    model = load_model(model_path)\n",
        "    model.eval()\n",
        "    \n",
        "    print(\"ðŸ§ª COMPREHENSIVE MODEL DIAGNOSTICS\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # 1. Full validation accuracy\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            # Move everything to GPU\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            preds = predict(model, images)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "            all_predictions.extend(preds.cpu().numpy())  # Move to CPU for storage\n",
        "            all_labels.extend(labels.cpu().numpy())      # Move to CPU for storage\n",
        "    \n",
        "    val_acc = correct / total\n",
        "    print(f\"1. Full Validation Accuracy: {val_acc:.4f} ({correct}/{total})\")\n",
        "    \n",
        "    # 2. Per-class accuracy (critical!)\n",
        "    from collections import Counter\n",
        "    class_correct = [0] * 15\n",
        "    class_total = [0] * 15\n",
        "    \n",
        "    for pred, label in zip(all_predictions, all_labels):\n",
        "        class_total[label] += 1\n",
        "        if pred == label:\n",
        "            class_correct[label] += 1\n",
        "    \n",
        "    print(\"\\n2. Per-Class Accuracy:\")\n",
        "    weak_classes = []\n",
        "    for i in range(15):\n",
        "        acc = class_correct[i] / class_total[i] if class_total[i] > 0 else 0\n",
        "        print(f\"   Class {i:2d}: {acc:.3f} ({class_correct[i]}/{class_total[i]})\")\n",
        "        if acc < 0.6:  # Classes under 60% are concerning\n",
        "            weak_classes.append(i)\n",
        "    \n",
        "    # 3. Prediction diversity test\n",
        "    unique_preds = len(Counter(all_predictions))\n",
        "    print(f\"\\n3. Prediction Diversity: {unique_preds}/15 classes predicted\")\n",
        "    \n",
        "    # 4. Batch-wise consistency test\n",
        "    print(\"\\n4. Batch Consistency Test:\")\n",
        "    for i in range(3):\n",
        "        test_images, test_labels = next(iter(val_loader))\n",
        "        test_images = test_images.to(device)\n",
        "        test_labels = test_labels.to(device)\n",
        "        preds = predict(model, test_images)\n",
        "        unique_in_batch = len(torch.unique(preds))\n",
        "        print(f\"   Batch {i}: {unique_in_batch} unique classes predicted\")\n",
        "\n",
        "        # Let's see what's actually in the batches\n",
        "        print(\"ðŸ” Let's examine the actual class distribution in batches:\")\n",
        "\n",
        "        for i in range(3):\n",
        "            test_images, test_labels = next(iter(val_loader))\n",
        "            unique_actual_classes = len(torch.unique(test_labels))\n",
        "            print(f\"Batch {i}: {unique_actual_classes} actual classes in the batch\")\n",
        "            \n",
        "    # 5. Final Assessment\n",
        "    print(\"\\n5. FINAL ASSESSMENT:\")\n",
        "    if val_acc >= 0.78 and unique_preds >= 12 and len(weak_classes) <= 3:\n",
        "        print(\"   âœ… EXCELLENT - High chance of test set success!\")\n",
        "        print(\"   Model is balanced, accurate, and predicts diverse classes\")\n",
        "    elif val_acc >= 0.75 and unique_preds >= 10:\n",
        "        print(\"   âœ… GOOD - Competitive model\")\n",
        "        print(\"   Should perform well on test set\")\n",
        "    else:\n",
        "        print(\"   âš ï¸  NEEDS IMPROVEMENT - Test set performance uncertain\")\n",
        "        if weak_classes:\n",
        "            print(f\"   Weak classes: {weak_classes}\")\n",
        "        if unique_preds < 10:\n",
        "            print(f\"   Only predicting {unique_preds}/15 classes\")\n",
        "    \n",
        "    return val_acc, weak_classes\n",
        "\n",
        "# Run the comprehensive test\n",
        "final_accuracy, weak_classes = comprehensive_model_test()\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EqTJ402xdTQ"
      },
      "source": [
        "implement Stochastic Gradient descent optimizer for lack of generaliztion: need momentum"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
